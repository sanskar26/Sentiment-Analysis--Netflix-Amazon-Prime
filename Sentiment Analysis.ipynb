{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hexaware_submission.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJ3hvlpCWgJ-"
      },
      "source": [
        "# Hexaware Submission (Senti-Group)\n",
        "\n",
        "###In this python notebook we have provided the code of how our model was trained and how data was collected for every dashboard that we have made"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9jGsticPOD2"
      },
      "source": [
        "## Install and Import necessary Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_cHOY2V7gxv"
      },
      "source": [
        "!pip install psycopg2-binary\n",
        "!pip install praw"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAGg61uc7cZ-"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Matplot\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Scikit-learn\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# Utility\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "\n",
        "# Praw\n",
        "import praw\n",
        "import datetime\n",
        "import sqlalchemy\n",
        "from sqlalchemy import create_engine\n",
        "from sqlalchemy.ext.declarative import declarative_base\n",
        "from sqlalchemy import Column, Integer, String, DateTime, Float\n",
        "from sqlalchemy.orm import sessionmaker\n",
        "\n",
        "# Tweepy\n",
        "import tweepy\n",
        "\n",
        "# TextBlob\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download(\"stopwords\")\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztcnDAf3vzNJ"
      },
      "source": [
        "##Sentiment Analysis Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qZoEgL1SWag"
      },
      "source": [
        "Define Network Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ecnCn64wpsQ"
      },
      "source": [
        "#NETWORK PARAMETERS\n",
        "TEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S+|[^A-Za-z0-9]+\"\n",
        "\n",
        "# KERAS\n",
        "SEQUENCE_LENGTH = 300\n",
        "EPOCHS = 10\n",
        "BATCH_SIZE = 1024\n",
        "\n",
        "# SENTIMENT\n",
        "POSITIVE = \"POSITIVE\"\n",
        "NEGATIVE = \"NEGATIVE\"\n",
        "NEUTRAL = \"NEUTRAL\"\n",
        "SENTIMENT_THRESHOLDS = (0.4, 0.7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QRLCl9WSa4k"
      },
      "source": [
        "Define Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flj2t1aywqEH"
      },
      "source": [
        "def map_sentiment(label):\n",
        "  decode_map = {0: \"NEGATIVE\", 4: \"POSITIVE\"}\n",
        "  return decode_map[int(label)]\n",
        "\n",
        "def clean(text, stem=False):\n",
        "  text = re.sub(TEXT_CLEANING_RE, ' ', str(text).lower()).strip()\n",
        "  tokens = []\n",
        "  for token in text.split():\n",
        "    tokens.append(token)\n",
        "  return \" \".join(tokens)\n",
        "\n",
        "def preprocess_data():\n",
        "  train = pd.read_csv('training.1600000.processed.noemoticon.csv',\n",
        "                    encoding=\"ISO-8859-1\",\n",
        "                    names=[\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
        "                    )\n",
        "  train = train.iloc[:,[0,5]]\n",
        "  train.target = train.target.apply(lambda x:map_sentiment(x))\n",
        "  train.text = train.text.apply(lambda x: clean(x))\n",
        "  df_train, df_test = train_test_split(train, test_size=0.2, random_state=42)\n",
        "  return df_train,df_test\n",
        "\n",
        "def tokenize(df_train):\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(df_train.text)\n",
        "  vocab_size = len(tokenizer.word_index)+1\n",
        "  return tokenizer,vocab_size\n",
        "\n",
        "def data2array(tokenizer):\n",
        "  x_train = pad_sequences(tokenizer.texts_to_sequences(df_train.text),\n",
        "                          maxlen=SEQUENCE_LENGTH)\n",
        "  x_test = pad_sequences(tokenizer.texts_to_sequences(df_test.text),\n",
        "                        maxlen=SEQUENCE_LENGTH)\n",
        "  encoder = LabelEncoder()\n",
        "  encoder.fit(df_train.target.tolist())\n",
        "  y_train = encoder.transform(df_train.target.tolist())\n",
        "  y_test = encoder.transform(df_test.target.tolist())\n",
        "  y_train = y_train.reshape(-1,1)\n",
        "  y_test = y_test.reshape(-1,1)\n",
        "  return x_train, x_test, y_train, y_test\n",
        "\n",
        "def get_callbacks(path):\n",
        "  callback1 = ReduceLROnPlateau(monitor='val_loss',patience=3, cooldown=0)\n",
        "  callback2 = EarlyStopping(monitor='val_accuracy', min_delta=1e-4, patience=5)\n",
        "  callback3 = ModelCheckpoint(path,\n",
        "                              monitor='val_loss',save_best_only=True)\n",
        "  callbacks = [callback1,callback2,callback3]\n",
        "  return callbacks\n",
        "\n",
        "def score(model,x_test,y_test):\n",
        "  score = model.evaluate(x_test,y_test,batch_size=BATCH_SIZE)\n",
        "  print()\n",
        "  print(\"ACCURACY:\",score[1])\n",
        "  print(\"LOSS:\",score[0])\n",
        "\n",
        "def plot_history(history):  #plot the trend in accuracy and loss\n",
        "  acc = history.history['accuracy']\n",
        "  val_acc = history.history['val_accuracy']\n",
        "  loss = history.history['loss']\n",
        "  val_loss = history.history['val_loss']\n",
        "  epochs = range(len(acc))\n",
        "  plt.plot(epochs,acc, 'b', label='Training accuracy')\n",
        "  plt.plot(epochs,val_acc, 'r', label='Validation accuracy')\n",
        "  plt.title('Training and Validation accuracy')\n",
        "  plt.legend()\n",
        "  plt.figure()\n",
        "  plt.plot(epochs,loss, 'b', label='Training loss')\n",
        "  plt.plot(epochs,val_loss, 'r', label='Validation loss')\n",
        "  plt.title('Training and Validation loss')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "def decode_sentiment(score, include_neutral=True):\n",
        "    if include_neutral:        \n",
        "        label = NEUTRAL\n",
        "        if score <= SENTIMENT_THRESHOLDS[0]:\n",
        "            label = NEGATIVE\n",
        "        elif score >= SENTIMENT_THRESHOLDS[1]:\n",
        "            label = POSITIVE\n",
        "        return label\n",
        "    else:\n",
        "        return NEGATIVE if score < 0.5 else POSITIVE\n",
        "\n",
        "def predict(text, include_neutral=True):\n",
        "  start_at = time.time()\n",
        "  text = clean(text)\n",
        "  x_test = pad_sequences(tokenizer.texts_to_sequences([text]),\n",
        "                         maxlen=SEQUENCE_LENGTH)\n",
        "  score = model.predict([x_test])[0]\n",
        "  label = decode_sentiment(score, include_neutral=include_neutral)\n",
        "  return {\"label\":label, \"score\": float(score),\n",
        "          \"elapsed_time\": time.time()-start_at}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSLNiYHoSfUT"
      },
      "source": [
        "Load Training data and Preprocess it in the form which we can feed into our Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p98YmiBawqew"
      },
      "source": [
        "!unzip training_data.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZ5bZGQTymzP"
      },
      "source": [
        "df_train, df_test = preprocess_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_wIyD6DynEc"
      },
      "source": [
        "tokenizer,vocab_size = tokenize(df_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agP0cqMvynMg"
      },
      "source": [
        "x_train,x_test,y_train,y_test = data2array(tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lzc9X_pHSqjs"
      },
      "source": [
        "Load our pretrained Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvB14exUywAM"
      },
      "source": [
        "model = keras.models.load_model('model_weights.h5')\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3Sk8gtES073"
      },
      "source": [
        "Train a new Model and provide path where you want model weights to be saved"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VdxNCfPQoJK"
      },
      "source": [
        "save_weights = 'model_weights_2.h5'\n",
        "callbacks = get_callbacks(save_weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbqpixmkywRb"
      },
      "source": [
        "history = model.fit(x_train,y_train,\n",
        "                    batch_size=BATCH_SIZE,\n",
        "                    epochs=EPOCHS,\n",
        "                    validation_split= 0.1,\n",
        "                    verbose=1,\n",
        "                    callbacks=callbacks\n",
        "                    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o68Otuz2SxVk"
      },
      "source": [
        "model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQBpI4ELy5mJ"
      },
      "source": [
        "score(model,x_test,y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdZNHqDIy524"
      },
      "source": [
        "plot_history(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TYnvhoZy-63"
      },
      "source": [
        "predict(\"i can't enjoy this\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xlrtr7Ev5rU"
      },
      "source": [
        "##Cross Platform Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWCJxImyTIQ1"
      },
      "source": [
        "Authenticate reddit api and provide Database URI "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVhXMsrxzUP6"
      },
      "source": [
        "reddit = praw.Reddit(client_id=\"\",\n",
        "                     client_secret=\"\",\n",
        "                     password=\"\",\n",
        "                     user_agent=\"\",\n",
        "                     username=\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXM1X6xZzUxf"
      },
      "source": [
        "DATABASE_URI = 'postgres://nskzjpswzdujzd:d0d7caf61b738c552483e356091a6a361bfc2207b6ddd5da993a85963f1dc366@ec2-52-202-22-140.compute-1.amazonaws.com:5432/db3js444ilgfce'\n",
        "engine = create_engine(DATABASE_URI)\n",
        "Base = declarative_base()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGwXFKr5TgnP"
      },
      "source": [
        "Define schema of our SQL table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2PSVqiczVME"
      },
      "source": [
        "class Comment(Base):\n",
        "  __tablename__ = 'comments'\n",
        "  id = Column(Integer, primary_key=True)\n",
        "  Comment = Column(String, nullable=False)\n",
        "  Created_at = Column(DateTime)\n",
        "  Sentiment = Column(String(10), nullable=False)\n",
        "  SentimentScore = Column(Float)\n",
        "  Platform = Column(String(20),nullable=False)\n",
        "  def __repr__(self):\n",
        "    return f\"<Comment(created_at={self.Created_at}, sentiment={self.Sentiment}, sentiment_score={self.SentimentScore})>\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKDqzEeKzVmm"
      },
      "source": [
        "Session = sessionmaker(bind=engine)\n",
        "session = Session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cYLkcFgTp4_"
      },
      "source": [
        "Stream the mentioned subreddit and capture any new comments posted there. These comments are then added to our online database"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cyhgouZ1MsH"
      },
      "source": [
        "platform = 'netflix'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hY0rzj5o0TVc"
      },
      "source": [
        "comments = reddit.subreddit(platform).stream.comments(skip_existing=True)\n",
        "i=0\n",
        "for comment in comments:\n",
        "  score = predict(comment.body)\n",
        "  user = Comment(Comment = comment.body, \n",
        "                 Created_at = datetime.datetime.fromtimestamp(comment.created), \n",
        "                 Sentiment = score['label'], \n",
        "                 SentimentScore = score['score'], \n",
        "                 Platform = str(platform).lower()\n",
        "                 )\n",
        "  session.add(user)\n",
        "  session.commit()\n",
        "  i+=1\n",
        "  print('Number of comments added: ',i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IeVGctjv7EP"
      },
      "source": [
        "##Genre-wise Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vn8R7u3xUC5i"
      },
      "source": [
        "Define Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKrFTrUF8Z43"
      },
      "source": [
        "def get_tweets(search,number,place):\n",
        "  tweets = tweepy.Cursor(api.search,q=search+'--place:%s'%place,lang='en').items(number)\n",
        "  return tweets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbHPscO5UGz2"
      },
      "source": [
        "Authenticate twitter api"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjaEgTzL5JQD"
      },
      "source": [
        "auth = tweepy.OAuthHandler('','')\n",
        "auth.set_access_token('','')\n",
        "api = tweepy.API(auth,wait_on_rate_limit=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoGjuhgf2kkY"
      },
      "source": [
        "countries = pd.read_excel('countries_with_place_ids.xlsx',dtype='str')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyTPK-QEUPei"
      },
      "source": [
        "Collect tweets for movies in a specific genre for all countries mentioned in our Excel file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYeVCUzr2lWi"
      },
      "source": [
        "sci_fi = pd.read_excel('SciFiMovieList.xlsx')\n",
        "genre = 'Sci-Fi'\n",
        "movie_list = [i for i in sci_fi.iloc[:,0]]\n",
        "number = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JqzuTCW2mgA"
      },
      "source": [
        "tweet_dict = []\n",
        "for movie in movie_list:\n",
        "  for i in range(len(countries)):\n",
        "    tweets = get_tweets(movie,number,countries.place_id[i])\n",
        "    tweet_dict.append({\n",
        "        'movie':movie,\n",
        "        'country':countries.Country[i],\n",
        "        'tweets':tweets   \n",
        "    }\n",
        "                      )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUpY5xZH2mFP"
      },
      "source": [
        "movie_genre = []\n",
        "country = []\n",
        "tweet_content = []\n",
        "creation_time = []\n",
        "user_age = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLgZMS1v5cvj"
      },
      "source": [
        "start = time.time()\n",
        "while(i<len(tweet_dict)):\n",
        "  tweets = tweet_dict[i]['tweets']\n",
        "  for tweet in tweets:\n",
        "    tweet_genre.append(genre)\n",
        "    tweet_movie.append(tweet_dict[i]['movie'])\n",
        "\n",
        "    tweet_country.append(tweet_dict[i]['country'])\n",
        "    tweet_text.append(tweet.text)\n",
        "    tweet_user_age.append(tweet.user.created_at)\n",
        "  print(len(tweet_dict)-i-1,'left')\n",
        "  i+=1\n",
        "print(time.time()-start)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOTH5rnLVCIO"
      },
      "source": [
        "Add those collected tweets and determine their sentiments. This data is then added to a pandas dataframe and saved in an Excel spreadsheet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omXO6TKs5iP5"
      },
      "source": [
        "genre_data = pd.DataFrame()\n",
        "genre_data['Genre'] = pd.Series(tweet_genre)\n",
        "genre_data['Movie'] = pd.Series(tweet_movie)\n",
        "genre_data['Country'] = pd.Series(tweet_country)\n",
        "genre_data['Tweet'] = pd.Series(tweet_text)\n",
        "genre_data['TwitterAge'] = pd.Series(tweet_user_age)\n",
        "\n",
        "genre_data['Sentiment'] = genre_data.Tweet.apply(lambda x:predict(x)['label'])\n",
        "genre_data['SentimentScore'] = genre_data.Tweet.apply(lambda x:predict(x)['score'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gLriSLk8GNd"
      },
      "source": [
        "genre_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVd6vTfF5kr9"
      },
      "source": [
        "genre_data.to_excel('SciFi_data.xlsx',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1OjM0auv7hw"
      },
      "source": [
        "##User Retention Analysis (13 Reasons Why)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVVEFVOfVHcC"
      },
      "source": [
        "Authenticate reddit api"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFqths1K73gr"
      },
      "source": [
        "reddit = praw.Reddit(client_id=\"\",\n",
        "                     client_secret=\"\",\n",
        "                     password=\"\",\n",
        "                     user_agent=\"\",\n",
        "                     username=\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvUILaCA38d3"
      },
      "source": [
        "comment_list = []\n",
        "created_at_list = []\n",
        "episode = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cgDCOh9VLjQ"
      },
      "source": [
        "Collect comments from reddit submissions meant for discussing 13 Reasons why"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EDPYOi53CaW"
      },
      "source": [
        "submission_url = 'https://www.reddit.com/r/13ReasonsWhy/comments/cuazdb/s3ep_13_let_the_dead_bury_the_dead/?utm_source=share&utm_medium=ios_app'\n",
        "season = 3\n",
        "episode_no = 13\n",
        "submission = reddit.submission(url=submission_url)\n",
        "submission.comments.replace_more(limit=0)\n",
        "submission.comment_sort = \"top\"\n",
        "comments = submission.comments.list()\n",
        "for comment in comments:\n",
        "  comment_list.append(comment.body)\n",
        "  created_at_list.append(datetime.datetime.fromtimestamp(comment.created))\n",
        "  episode.append(str(season)+'.'+str(episode_no))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3Rj-33dWC8q"
      },
      "source": [
        "Determine Sentiment score for collected data and save this to an Excel Spreadsheet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVZs0_k13Chx"
      },
      "source": [
        "reddit_comments = pd.DataFrame()\n",
        "reddit_comments['Comment'] = pd.Series(comment_list)\n",
        "reddit_comments['Created_at'] = pd.Series(created_at_list)\n",
        "reddit_comments['Episode'] = pd.Series(episode)\n",
        "reddit_comments['Sentiment'] = reddit_comments.Comment.apply(lambda x:predict(x)['label'])\n",
        "reddit_comments['SentimentScore'] = reddit_comments.Comment.apply(lambda x:predict(x)['score'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcfxHfvX8DvA"
      },
      "source": [
        "reddit_comments"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gF6N50T3Cpw"
      },
      "source": [
        "reddit_comments.to_excel('13ReasonsWhy.xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2JmhYzE9qP7"
      },
      "source": [
        "##Analysis using Word Frequency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xvst1Y1jeDOD"
      },
      "source": [
        "def get_tuple(start,end,n):\n",
        "  x = [a for a in range(start,end+1)]\n",
        "  y = []\n",
        "  for i in range(end-n+1):\n",
        "    y.append([x[j] for j in range(i,i+n)])\n",
        "  return y\n",
        "\n",
        "def get_word_freq(ngram,df_new):\n",
        "  wt_words = []\n",
        "  for i in range(len(df_new)):\n",
        "    current_grams = []\n",
        "    x = clean(df_new.loc[i].Comment).split(' ')\n",
        "    indices = get_tuple(0,len(x),ngram)\n",
        "    for y in indices:\n",
        "      lists = ''\n",
        "      for index in y:\n",
        "        if x[index] in stop_words:\n",
        "          continue\n",
        "        lists += x[index] + ' '\n",
        "      if len(lists.split())==ngram:\n",
        "        current_grams.append(lists)\n",
        "    wt_words += current_grams\n",
        "  data_analysis = nltk.FreqDist(wt_words)\n",
        "  filter_words = dict([(m, n) for m, n in data_analysis.items()])\n",
        "  data_analysis = nltk.FreqDist(filter_words)\n",
        "  data_analysis.plot(25, cumulative=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJA9uNQn9pze"
      },
      "source": [
        "df = pd.read_excel('/content/sentiment_allplatforms_reddit.xlsx')\n",
        "\n",
        "filter_words = ['hulu','netflix', 'season', 'black','reality', 'fun', 'prime', 'movie', 'show', 'get', 'would', 'one', 'streaming', 'also']\n",
        "for x in filter_words :\n",
        "  stop_words.add(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6jlcqEFAhA3"
      },
      "source": [
        "sentiments = ['POSITIVE','NEGATIVE','NEUTRAL']\n",
        "for i in range(3):\n",
        "  for n_gram in [1,2,3]:\n",
        "    print(f'{sentiments[i]}  {n_gram}-Grams')\n",
        "    df_new = df.query(f'Sentiment==\"{sentiments[i]}\"')\n",
        "    df_new.reset_index(drop=True, inplace=True)\n",
        "    get_word_freq(n_gram,df_new)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}